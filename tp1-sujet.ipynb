{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "***\n",
    "**Algorithmes d'optimisation -- L3 MINT et doubles licences 2018/2019 -- Université Paris-Sud**\n",
    "***\n",
    "\n",
    "# TP 1: Minimisation de fonctionnelles quadratiques convexes\n",
    "\n",
    "$\\newcommand{\\Rsp}{\\mathbb{R}}\n",
    "\\newcommand{\\nr}[1]{\\|#1\\|}\n",
    "\\newcommand{\\abs}[1]{|#1|}\n",
    "\\newcommand{\\eps}{\\varepsilon}\n",
    "\\newcommand{\\sca}[2]{\\langle#1|#2\\rangle}\n",
    "\\newcommand{\\D}{\\mathrm{D}}\n",
    "\\newcommand{\\hdots}{\\dots}\n",
    "\\newcommand{\\cond}{\\mathrm{cond}}$\n",
    "\n",
    "## Préliminaires\n",
    "Dans ce TP, on s'intéresse à la minimisation sur $\\Rsp^N$ de fonctionnelles de la forme suivante,\n",
    "\n",
    "$$ \n",
    "\\begin{aligned}\n",
    "f(x) &= \\frac{1}{2} \\sca{x}{Q x} + \\sca{b}{x} \\\\\n",
    "&= \\frac{1}{2} \\sum_{1\\leq i\\leq j \\leq N} Q_{ij} x_i x_j + \\sum_{1\\leq i\\leq N} b_i x_i,\n",
    "\\end{aligned} $$\n",
    "\n",
    "où $Q$ est une matrice symétrique définie positive et $b$ est un vecteur colonne. Notre objectif principal est de constater numériquement un phénomène expliqué en cours, à savoir que l'efficacité des méthodes de descente de gradient dépend crucialement du *conditionnement* de la matrice $Q$ (voir ci-dessous pour une définition).\n",
    "\n",
    "**Quelques commentaires sur les Notebook.**\n",
    "Ce texte est rédigé sous la forme d'un notebook. Un notebook comporte des cellules de texte et des cellules de code, ici en Python. Quelques raccourcis clavier et remarques utiles:\n",
    "\n",
    "- `CTRL+Entrée`: exécute la cellule de code, et affiche son résultat.\n",
    "- `Tab`: Si l'on `Tab` après avoir tapé les premières lettres d'un nom de fonction, le système propose une liste de possibilités (ce qui peut permettre d'éviter des erreurs de frappe)\n",
    "- `MAJ+Tab`: Affiche la documentation sur la fonction. Très utile pour ne pas se tromper sur l'ordre des paramètres. On peut voir une documentation plus complète en cliquant sur le '+'.\n",
    "- `CTRL+s`: Enregistrer les modifications apportées au Notebook.\n",
    "- Le symbole `[*]` à côté d'une cellule de code indique que le noyau Python est toujours en train de calculer. On peut l'interrompre via `Kernel -> Interrupt` ou le redémarrer via `Kernel -> Restart`. Le noyau Python repart alors de zéro, et il faut donc relancer les cellules antérieures à celle sur laquelle on travaillait.\n",
    "\n",
    "Une aide complète, ainsi que la documentation de Python et Numpy, est disponible dans le menu `Aide`.\n",
    "\n",
    "**Rappels de cours et du TD précédent.**\n",
    "Ce TP ne nécessite que quelques définitions et théorème du cours et des cours précédents, que l'on rappelle ici (le théorème de convergence ci-dessous sera démontré un peu plus tard, dans un cas plus général).\n",
    "\n",
    "> **Proposition:** Une fonction $f\\in\\mathcal{C}^2(\\Rsp^d)$ est convexe si pour tout $x\\in\\Rsp^d$, $D^2 f(x)$ est une matrice symétrique positive, i.e. $\\forall x\\in\\Rsp^d,\\forall v\\in \\Rsp^d, \\sca{v}{D^2 f(x) v} \\geq 0$.\n",
    "\n",
    "> **Proposition:** Si $f \\in \\mathcal{C}^1(\\Rsp^d)$ est convexe, alors $x^* = \\arg\\min_{x\\in \\Rsp^d} f(x) \\Longleftrightarrow \\nabla f(x^*) = 0. $\n",
    "\n",
    "> **Théorème:** Toute matrice symétrique $Q$ est diagonalisable dans une base orthonormale. En d’autres mots, il existe une matrice orthogonale P telle que $ P^TQP$\n",
    "soit diagonale.\n",
    "\n",
    "> **Définition:** On appelle *conditionnement* d'une matrice symétrique définie positive $Q\\in M_N(\\Rsp)$ de valeurs propres $0< \\lambda_1\\leq \\dots \\leq \\lambda_N$ la quantité $\\cond(Q) = \\lambda_N / \\lambda_1$. \n",
    "\n",
    "> **Théorème:** Soit $f(x) = \\frac{1}{2} \\sca{x}{Q x} + \\sca{b}{x}$ où $Q$ est une matrice symétrique définie positive, et soient  $(x^{(k)})_{k\\geq 0}$ les itérées de l'algorithme de descente de gradient à pas optimal, c'est à dire $x^{(0)} \\in \\Rsp^d$ et \n",
    "$$ \\begin{cases}\n",
    "d^{(k)} = \\nabla f_K(x^{(k)})\\\\\n",
    "t^{(k)} = \\arg\\min_{t} f(x^{(k)} + t d^{(k)})  &\\hbox{ pour $k\\geq 0$}\\\\\n",
    "x^{(k+1)} = x + t^{(k)} d^{(k)}.\n",
    "\\end{cases}\n",
    "$$\n",
    "alors, avec $x^* = \\arg\\min_x f(x)$ et $c = 1 - \\cond(Q)^{-1} < 1$, on a\n",
    "$$ f(x^{(k+1)}) - f(x^*) \\leq c(f(x^{(k)}) - f(x^*)).$$\n",
    "\n",
    "## I. Gradient à pas optimal en dimension $N=2$ \n",
    "\n",
    "Pour commencer, on commence par considérer la minimisation de la fonction $f_K:\\Rsp^2\\to\\Rsp$ définie par $f_K(x) = \\frac{1}{2}Kx_1^2 + \\frac{1}{2}x_2^2$ où $K$ est une constante strictement positive.\n",
    "\n",
    "**QI.1.** Calculer le gradient et la matrice hessienne de $f_K$, montrer que $f_K$ est convexe. Montrer que son unique minimiseur sur $\\Rsp^2$ est $x^* = (0,0)$.\n",
    "\n",
    "\n",
    "\n",
    "**QI.2.** Étant donné un point $x^{(k)} = (x_1^{(k)},x_2^{(k)}) \\in \\Rsp^2$ et $d^{(k)} = -\\nabla f_K(x)$, calculer le pas  optimal $t^{(k)}$. \n",
    "\n",
    "*(Indication: On pourra utiliser la formule générale pour le pas donnée en partie II pour une fonction de la forme $f(x) = \\frac{1}{2}\\sca{x}{Qx} + \\sca{b}{x}$.)*\n",
    "\n",
    "\n",
    "\n",
    "**QI.3.** Programmer l'algorithme du gradient à pas optimal, et le tester  pour $K=2$. On arrêtera les itérations dès que $f_K(x^{(k)}) \\leq f_K(x^*) + 10^{-8}$.  Tracer deux figures :\n",
    "* la trajectoire des itérées $(x_1^{(k)}, x_2^{(k)})_{k\\geq 1}$ (via la fonction `plt.plot`)\n",
    "* l'évolution de l'erreur $\\log(f(x^{(k)}) - f(x^*))$ en fonction de $k$.\n",
    "Que se passe-t-il lorsque l'on change $K$ ?\n",
    "\n",
    "(*Indication: On pourra stocker la liste des itérées $x^{(k)}$ et des valeurs $f(x^{(k)})$ dans deux listes `X` et `F`, et utiliser la fonction `X.append(...)` pour ajouter un élément à la liste `X`. Pour l'erreur, on recommande la fonction `plt.semilogy`.*)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on importe les modules numpy et pyplot\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# les deux commandes suivante paramètrent l'affichage des figures\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = [9.,6.]\n",
    "\n",
    "# initialiser les variables K,Q, x0, fmin\n",
    "K = 2.\n",
    "X = [] # tableau pour stocker la liste des itérées x^k\n",
    "F = [] # tableau pour stocker la liste des valeurs f(x^k)\n",
    "x = np.array([1.,K]) \n",
    "\n",
    "# <completer>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "**QI.4.** Recommencer l'expérience en choisissant $K = 10,100,500$. Calculer le taux de décroissance moyen (c'est-à-dire la moyenne de $(f_K(x^{(k+1)})-f(x^*))/(f_K(x^{(k)}) - f(x^*)$) et le comparer à la borne donnée dans le théorème de convergence rappelé en préliminaire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# <completer>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "## II. Gradient à pas optimal pour un problème de moindres carrés \n",
    "\n",
    "On commence par écrire l'algorithme de gradient à pas optimal pour minimiser sur   $\\Rsp^N$ une fonction de la forme $$f(x) = \\frac{1}{2} \\sca{Q x}{x} + \\sca{b}{x}$$\n",
    "où $Q$ est une matrice $N\\times N$ symétrique définie positive et $b\\in\\Rsp^N$ est un vecteur.\n",
    "En TD, nous avons vu que :\n",
    "* $\\nabla f(x) = Qx + b$, $\\mathrm{D}^2 f(x) = Q$\n",
    "* $f$ est convexe, et si $x^*$ est le minimiseur de $f$ sur $\\Rsp^N$, alors il vérifie l'équation $\\nabla f(x^*) = Qx^*+b = 0$.\n",
    "* l'algorithme de descente de gradient à pas optimal s'écrit \n",
    "$$ \\begin{cases}\n",
    "d^{(k)} = \\nabla f_K(x^{(k)})\\\\\n",
    "t^{(k)} = \\frac{\\sca{d^{(k)}}{d^{(k)}}}{\\sca{d^{(k)}}{Qd^{(k)}}}.  &\\hbox{ pour $k\\geq 0$}\\\\\n",
    "x^{(k+1)} = x^{(k)} + t^{(k)} d^{(k)}.\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "**QII.1** Programmer une fonction `gradient_optimal(Q,b,x0,err)`, prenant en argument la matrice $Q$, le vecteur $b$ et le point de départ $x^{(0)}$, qui calculera les itérées de la méthode de descente de gradient à pas optimal (cf préliminaires). De plus,\n",
    "* Les itérations seront interrompues lorsque $\\nr{d^{(k)}} \\leq $ `err` (on pourra utiliser la fonction `np.linalg.norm`) ou dès que $k>10^6$.\n",
    "* La fonction retournera le dernier point $x^{(k)}$ trouvé ainsi que deux vecteurs $E,F$ tels que $E^{(k)} = \\nr{d^{(k)}}$ et $F^{(k)} = f(x^{(k)})$. \n",
    "\n",
    "Tester cette fonction avec une matrice $Q = A^T A + \\mathrm{Id}$, où $A$ est une matrice aléatoire et $b$ est un vecteur aléatoire, pour $N=10$. On comparera la solution construite par l'algorithme de descente de gradient à la solution exacte $x^* = -Q^{-1}b$ retournée par `-np.linalg.solve(Q,b)`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_optimal(Q,b,x0,err=1e-6):\n",
    "    x = x0\n",
    "    niter=0\n",
    "    E = []\n",
    "    F = []\n",
    "    \n",
    "    k = 0 # nombre d'itérations\n",
    "    while (True): \n",
    "        k = k+1\n",
    "        if k > 1e6: # maximum de 10^6 itérations\n",
    "            print('erreur: nombre maximum d\\'itérations atteint')\n",
    "            break\n",
    "        # calculer la direction de descente\n",
    "        # <completer>\n",
    "        # vérifier le critère d'arrêt, et quitter la boucle (avec break) s'il est vérifié\n",
    "        # <completer>\n",
    "        # calculer le pas de descente et mettre à jour x\n",
    "        # <completer>\n",
    "    E = np.array(E)\n",
    "    F = np.array(F)\n",
    "    return x,E,F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test de la fonction gradient_optimal pour un Q,b aléatoire\n",
    "N = 10\n",
    "A = np.random.randn(N,N)\n",
    "Q = np.dot(A.T,A)+np.eye(N,N)\n",
    "b = np.random.rand(N)\n",
    "# <completer>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "**Régression linéaire:** On possède un jeu de donnée constitué de points $(x_i,y_i)_{1\\leq i\\leq n} \\in\\Rsp^2$ et on cherche à trouver $(\\mu_0, \\mu_1) \\in\\Rsp^2$ minimisant la fonction \n",
    "\n",
    "$$ f: \\mu\\in\\Rsp^2\\mapsto \\frac{1}{2} \\sum_{1 \\leq i \\leq n} |\\mu_0 + \\mu_1 x_i - y_i|^2. $$\n",
    "\n",
    "Ce problème d'optimisation peut être interprété de de la manière suivante. Étant donnée une abscisse à l'origine $\\mu_0$ et une  pente $\\mu_1$, on peut calculer la distance euclidienne  entre la donnée $(x_i,y_i)$ et le point $(x_i,\\mu_0 + \\mu_1 x_i)$ qui appartient à la droite \n",
    "$\\mathcal{D} = \\{ (x,y) \\mid y = \\mu_0 + \\mu_1 x\\}$, soit\n",
    "$$\\eps_i = |\\mu_0 + \\mu_1 x_i - y_i|$$\n",
    "Le problème d'optimisation précédent consiste à trouver une droite d'abscisse à l'origine $\\mu_0$ et de pente $\\mu_1$ minimisant la somme des erreurs $\\eps_1^2 + \\hdots + \\eps_n^2$, et passant donc au plus près du jeu de données. Cette méthode a été inventée simultanément par Gauss et [Legendre](http://www.bibnum.education.fr/sites/default/files/legendre-texte.pdf).\n",
    "\n",
    "Pour nos expériences, les points de données $(x_i,y_i)$ sont construits de la manière suivante:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f52eab72320>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f52eac404e0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n=50\n",
    "X = np.linspace(-1,1,n);\n",
    "Y = np.sin(np.pi*X) + .1*np.random.rand(n)\n",
    "plt.plot(X,Y,'.',label='donnees')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "**QII.2** Construire une matrice $A$ ayant $n$ lignes et $2$ colonnes telle que le vecteur $Z := (\\mu_0 + \\mu_1 x_i)_{1\\leq i\\leq n}$ s'écrive $Z = A\\mu$ (produit matrice vecteur). Remarquer que l'on peut alors écrire $f(\\mu) = \\frac{1}{2} \\nr{A \\mu - Y}^2$ où $Y = (y_i)_{1\\leq i\\leq n}$.\n",
    "\n",
    "\n",
    "**Remarque :** En TD, nous avons vu que la fonction $f(\\mu) = \\nr{A \\mu - Y}^2$ peut être mise sous la forme $$f(\\mu)=\\frac{1}{2}\\sca{\\mu}{Q \\mu} + \\sca{b}{\\mu} + c,$$\n",
    "où $Q = A^T A$, $b = - A^T Y$ et $c = \\frac{1}{2}Y^T Y$.\n",
    "On a aussi montré que si la matrice $A$ injective alors $Q$ est symétrique définie positive.\n",
    "\n",
    "**QII.3** Calculer le minimum  $\\mu^*$ de $f$ à l'aide de la fonction `gradient_optimal`, tracer sur une même figure les points $(X,Y)$ et la droite $(X, A\\mu^*)$. Vérifier la correction du résultat en le comparant à celui obtenu en résolvant le système linéaire $\\nabla f(\\mu^*) = 0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculer A, Q, b, puis appliquer l'algorithme du gradient à pas optimal\n",
    "# <completer>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "**Régression polynômiale** On s'intéresse maintenant à une généralisation du problème de régression linéaire. Il s'agit cette fois-ci d'approcher au mieux le jeu de données $(x_i,y_i)_{1\\leq i\\leq n}$ par des points de la forme $(x_i,P_\\mu(x_i))_{1\\leq i\\leq N}$ où $P_\\mu(X) = \\sum_{0\\leq i\\leq d} \\mu_i X^i$ est un polynôme de degré $d$ à déterminer. L'inconnue de notre problème est donc le vecteur $\\mu \\in \\Rsp^{d+1}$, qu'on choisit via le problème d'optimisation suivant:\n",
    "$$ \\min_{\\mu \\in \\Rsp^{d+1}} \\frac{1}{2} \\sum_{1\\leq i\\leq N} \\nr{P_\\mu(x_i) - y_i}^2, $$\n",
    "\n",
    "**QII.4** Montrer que ce problème est équivalent au problème suivant:\n",
    "$$ \\min_{(\\mu_0,\\hdots,\\mu_d) \\in \\Rsp^{d+1}} f_d(\\mu) := \\frac{1}{2} \\nr{A_d \\mu - Y}^2 \\quad \\hbox{ où } A_d = \\begin{pmatrix} 1 & x_1 & x_1^2 & \\hdots & x_1^d \\\\\n",
    "1 & x_2 & x_2^2 & \\hdots & x_2^d \\\\\n",
    "\\vdots & \\vdots & \\vdots & &\\vdots \\\\\n",
    "1 & x_n & x_n^2 & \\hdots & x_n^d,\n",
    "\\end{pmatrix}$$\n",
    "puis que $f_d$ est convexe et même strictement convexe si les points $x_i$ sont distincts deux à deux.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**QII.5** Résoudre le problème d'optimisation pour $2\\leq d \\leq 7$ via la fonction `gradient_optimal`, en fixant `err=1e-4`. Interpréter l'accroissement du nombre d'itérations de l'algorithme en calculant le conditionnement des matrices $A_d$ via la fonction `np.linalg.cond`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on commence par construire la matrice A pour d=2, en guise d'exemple:\n",
    "A = np.vstack([X**0, X**1, X**2]).T\n",
    "# ou, de manière équivalente\n",
    "A = np.vstack([X**i for i in range(3)]).T\n",
    "\n",
    "# <completer>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "**QII.6** On pose $g_d(\\mu) = f_d(\\mu) + \\frac{\\gamma}{2}\\sum_{0\\leq i\\leq d} \\mu_i^2$. Mettre $g_d$ sous la forme \n",
    "$$ g_d(\\mu) := \\frac{1}{2} \\sca{\\mu}{R_d \\mu} + \\sca{b_d}{T} \\mu + c_d $$\n",
    "et démontrer que $\\cond(R_d) < \\cond(Q_d)$ où $Q_d:=A^T_d A_d$. Vérifier que l'algorithme du gradient à pas optimal converge plus rapidement.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# <completer>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "## III Exercice (valeurs propres et conditionnement)\n",
    "\n",
    "**QIII.1** Soit $Q$ une matrice symétrique de taille $d\\times d$ et $\\lambda_1\\leq \\hdots\\leq \\lambda_d$ ses valeurs propres. \n",
    "Démontrer que pour tout $x \\in \\Rsp^d$, \n",
    "$$ \\lambda_1 \\nr{x}^2 \\leq \\sca{x}{Qx} \\leq \\lambda_d \\nr{x}^2. $$\n",
    "En déduire que \n",
    "$$ \\lambda_1 = \\min_{x \\neq 0} \\frac{\\sca{x}{Qx}}{\\nr{x}^2} \\qquad \n",
    "\\lambda_d = \\max_{x \\neq 0} \\frac{\\sca{x}{Qx}}{\\nr{x}^2} $$\n",
    "\n",
    "\n",
    "\n",
    "**QIII.2** On considère maintenant la matrice $Q_d = A_d^T A_d$ où $A_d$ est définie en QII.6. \n",
    "En considérant $\\mu = (1,0,\\hdots,0)$ (resp. $\\mu = (0,\\hdots,0,1)$) \n",
    "démontrer que \n",
    "$$\\lambda_d \\geq n \\qquad (\\hbox{resp. } \\lambda_1 \\leq \\sum_{1\\leq i\\leq n} x_i^{2d})$$\n",
    "En déduire une minoration de $\\cond(Q_d)$ ne faisant intervenir que $n$ et $d$.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "None",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
