{"cells": [{"cell_type": "markdown", "metadata": {"deletable": false}, "source": ["***\n", "**Algorithmes d'optimisation -- L3 MINT et doubles licences 2019/2020 -- Universit\u00e9 Paris-Saclay**\n", "***\n", "$\\newcommand{\\Rsp}{\\mathbb{R}} \\newcommand{\\nr}[1]{\\|#1\\|} \\newcommand{\\abs}[1]{|#1|} \\newcommand{\\eps}{\\varepsilon} \\newcommand{\\sca}[2]{\\langle#1|#2\\rangle} \\newcommand{\\D}{\\mathrm{D}} \\newcommand{\\hdots}{\\dots} \\newcommand{\\cond}{\\mathrm{cond}}$\n", "On commence par importer les modules habituels:"]}, {"cell_type": "code", "execution_count": 8, "metadata": {}, "outputs": [], "source": ["import numpy as np\n", "import matplotlib.pyplot as plt\n", "%matplotlib inline\n", "# la commande suivante agrandit les figures\n", "plt.rcParams['figure.figsize'] = [9.,6.]\n", "\n", "def verifier_gradient(f,g,x0):\n", "    N = len(x0)\n", "    gg = np.zeros(N)\n", "    for i in range(N):\n", "        eps = 1e-4\n", "        e = np.zeros(N)\n", "        e[i] = eps\n", "        gg[i] = (f(x0+e) - f(x0-e))/(2*eps)\n", "    print('erreur numerique dans le calcul du gradient: %g (doit etre petit)' % np.linalg.norm(g(x0)-gg))"]}, {"cell_type": "markdown", "metadata": {"deletable": false}, "source": ["# TP 3: Gradient projet\u00e9 et p\u00e9nalisation \n", "\n", "## Partie I: Probl\u00e8me d'obstacle par gradient projet\u00e9\n", "\n", "On consid\u00e8re un syst\u00e8me physique constitu\u00e9 d'une chaine de $N+1$ ressorts. Les deux extr\u00e9mit\u00e9s du $i$\u00e8me ressort ($0\\leq i\\leq N$ sont les points $(t_i,x_i) \\in\\Rsp^2$ et $(t_{i+1},x_{i+1}) \\in \\Rsp^2$, o\u00f9 $t_i = hi$ est fix\u00e9.  On consid\u00e8re \u00e9galement que la chaine est fix\u00e9e \u00e0 ses extr\u00e9mit\u00e9s: $x_0 = x_{N+1} = 0$. Il reste donc $N$ inconnues $x = (x_1,\\hdots,x_N)\\in \\Rsp^N$. L'\u00e9nergie du syst\u00e8me est donn\u00e9e par la formule suivante:\n", "\n", "$$J(x) = J(x_1,\\hdots,x_N) = \\frac{1}{2}\\sum_{0\\leq i\\leq N} \\nr{x_{i+1} - x_i}^2 $$ \n", "\n", "o\u00f9 l'on a donc fix\u00e9 $x_0 = x_{N+1} = 0$. On pose un obstacle sous la cha\u00eene de ressort, qui force chacune des coordonn\u00e9es $x_i$ a \u00eatre minor\u00e9e par une constante $f_i$ (on peut penser \u00e0 une main qui pousse le ressort par exemple). On arrive dont au probl\u00e8me d'optimisation\n", "\n", "$$ \\min_{x\\in K} J(x) \\hbox{ o\u00f9 } K = \\{x\\in \\Rsp^N \\mid \\forall 1\\leq i\\leq N, x_i \\geq f_i \\}. $$\n", "\n", "Nous allons la r\u00e9solution num\u00e9rique de ce probl\u00e8me d'optimisation par la m\u00e9thode de gradient projet\u00e9. On rappelle les formules suivantes:\n", "\n", "- La projection d'un point $y\\in\\Rsp^N$ sur le convexe ferm\u00e9 $K$ est donn\u00e9e par \n", "\n", "$$ P_K(y) = (\\max(y_1,f_1),\\hdots,\\max(y_N,f_N)) $$\n", "\n", "- On admet que la fonction $J$ peut \u00eatre mise sous la forme\n", "\n", "$$ J(x) = \\frac{1}{2} \\sca{x}{Q x}  \\hbox{ o\u00f9 } Q = \\begin{pmatrix}2 & -1  & 0 &\\cdots & 0 \\\\\n", "-1 & 2 & -1 & \\ddots & \\vdots   \\\\ \n", "0 & \\ddots & \\ddots & \\ddots& \\vdots \\\\\n", "\\vdots & \\ddots & -1 & 2 & -1 \\\\\n", "0 & \\hdots & 0 & -1 & 2\n", "\\end{pmatrix}$$\n", "\n", "Le gradient de la fonction  $J$ en $x\\in \\Rsp^N$ est alors donn\u00e9 par $\\nabla J(x) = Q x.$\n", "\n", "**Q1)** \u00c9crire une fonction projK(x) retournant la projection de $x\\in \\Rsp^N$ sur $K$ (la tester avec des petites valeurs de $N$). On fixe d\u00e9sormais $N=30$: \u00e9crire la matrice $Q$ et deux fonctions `J(x)` et `gradJ(x)` calculant la valeur et le gradient de $J$. V\u00e9rifier le calcul du gradient avec `check_gradient`."]}, {"cell_type": "code", "execution_count": 27, "metadata": {}, "outputs": [], "source": ["# initialiser les variables K,Q, x0, fmin\n", "\n", "N = 30\n", "T = np.linspace(0,1,N+2)[1:-1]\n", "F = np.exp(-50*(T-.75)**2) + np.exp(-50*(T-.25)**2)\n", "#plt.plot(T,F,'.-g',label='obstacle')\n", "plt.fill_between(T,F,'.-g',label='obstacle',alpha=0.1)\n", "plt.legend()\n", "\n", "# <completer>\n"]}, {"cell_type": "markdown", "metadata": {"deletable": false}, "source": ["On rappelle que l'algorithme du gradient projet\u00e9 est d\u00e9fini de la mani\u00e8re suivante:\n", "\n", "$$ \\begin{cases}\n", "x^{(0)} \\in \\Rsp^N\\\\\n", "x^{(k+1)} = P_K\\left(x^{(k)} - \\tau \\nabla J(x^{(k)})\\right)\n", "\\end{cases} $$\n", "\n", "o\u00f9 $\\tau>0$. On a d\u00e9montr\u00e9 en cours que si \n", "\n", "$$ \\forall x\\in \\Rsp^M, m\\mathrm{Id} \\leq \\D^2 J(x) \\leq M\\mathrm{Id} , $$\n", "\n", "alors l'algorithme converge d\u00e8s que $\\tau < 2m/M^2$, avec un taux optimal lorsque $\\tau^* = m/M^2$.\n", "\n", "**Q2)** Montrer que $\\D^2 J(x) = Q$; calculer les valeurs propres de $Q$ via la fonction np.linalg.eigvalsh. Quelle valeur de $\\tau^*$ l'estimation ci-dessus donne-t-elle? "]}, {"cell_type": "code", "execution_count": 10, "metadata": {}, "outputs": [], "source": ["# <completer>\n"]}, {"cell_type": "markdown", "metadata": {"deletable": false}, "source": ["**Q3)** \u00c9crire une boucle r\u00e9alisant l'algorithme du gradient projet\u00e9 (pour $1\\leq k\\leq 500$), et stockant le vecteur $G=(\\nr{x^{(k+1)} - x^{(k)}})$ (comme il s'agit d'un algorithme de point fixe, c'est une bonne mani\u00e8re de quantifier la convergence). Tracer la solution $x \\in \\Rsp^N$ trouv\u00e9e toutes les 20 it\u00e9rations (on sugg\u00e8re plt.plot(T,x)).  Tester pour $\\tau = \\tau^*$. En pratique, v\u00e9rifier que l'algorithme converge toujours $\\tau=0.5$ mais diverge pour $\\tau$ trop grand."]}, {"cell_type": "code", "execution_count": 26, "metadata": {}, "outputs": [], "source": ["# <completer>\n"]}, {"cell_type": "markdown", "metadata": {"deletable": false}, "source": ["**Q4**) Comme $\\nabla J(x) = Qx$, l'algorithme peut en fait \u00eatre d\u00e9crit par\n", "\n", "$$ \\begin{cases}\n", "x^{(0)} \\in \\Rsp^N\\\\\n", "x^{(k+1)} = P_K(A_\\tau x^{(k)}) \n", "\\end{cases} $$\n", "\n", "o\u00f9 $A_\\tau = \\mathrm{Id}_N - \\tau Q$. Montrer que si toutes les valeurs propres de $A_\\tau$ sont de module $<1$, alors l'application $x\\mapsto A_\\tau x$ est contractante. V\u00e9rifier num\u00e9riquement ce crit\u00e8re pour $\\tau=0.5$ (on utilisera np.linalg.eigvalsh pour calculer les valeurs propres)."]}, {"cell_type": "code", "execution_count": 5, "metadata": {}, "outputs": [], "source": ["# <completer>\n"]}, {"cell_type": "markdown", "metadata": {"deletable": false}, "source": ["# Probl\u00e8me d'obstacle par p\u00e9nalisation\n", "\n", "Une autre mani\u00e8re d'approcher la solution d'un probl\u00e8me d'optimisation sous contrainte consiste \u00e0 *p\u00e9naliser* la violation des contraintes. Plus pr\u00e9cis\u00e9ment, on rajoute \u00e0 la fonctionnelle $J$ un terme par contrainte d'in\u00e9galit\u00e9 $x_i >= F_i$, de la forme $P_i(x) = \\frac{1}{\\eps} \\max(F_i - x_i, 0)^2$ o\u00f9 $\\eps>0$: \n", "\n", "- Si le point $x = (x_1,\\hdots,x_N)$ v\u00e9rifie $x_i\\geq F_i$, alors $P_i(x) = 0$. \n", "- Par contre, si $x_i < F_i$ (et ne satisfait pas la contrainte), alors $P_i(x) \\geq \\frac{1}{\\eps} (F_i - x_i)^2$\n", "\n", "Ainsi, on consid\u00e8re le probl\u00e8me de minimisation\n", "\n", "$$P_\\eps := \\min_{x\\in\\Rsp^N} J_\\eps(x) \\hbox{ o\u00f9 } J_\\eps(x) = J(x) + \\frac{1}{\\eps} \\sum_{1\\leq i\\leq N} \\max(F_i - x_i,0)^2. $$\n", "\n", "on admettra que \n", "\n", "$$\\nabla J_\\eps(x) = \\nabla J(x) - \\frac{2}{\\eps} \\sum_{1\\leq i\\leq N} \\max(F_i - x_i, 0) e_i $$\n", "\n", "\n", "**Q1)** \u00c9crire deux fonctions `Jeps`/`gradJeps` calculant $J_\\eps/\\nabla J_\\eps$. V\u00e9rifier le calcul du gradient en utilisant `verifier_gradient`."]}, {"cell_type": "code", "execution_count": 14, "metadata": {}, "outputs": [], "source": ["def Jeps(x,eps):\n", "    # <completer>\n", "    \n", "def gradJeps(x,eps):\n", "    # <completer>\n", "\n", "x0 = np.random.rand(N)\n", "verifier_gradient(lambda x: Jeps(x,1), \n", "                  lambda x: gradJeps(x,1),\n", "                  x0)\n"]}, {"cell_type": "markdown", "metadata": {"deletable": false}, "source": ["**Q2)** R\u00e9soudre le probl\u00e8me $P_\\eps$ par descente de gradient avec rebroussement (on fournit la fonction `gradient_armijo`) pour $\\eps=1,0.1,0.01,0.001$. Interpr\u00e9ter l'explosion du nombre d'it\u00e9rations."]}, {"cell_type": "code", "execution_count": 25, "metadata": {}, "outputs": [], "source": ["def rebroussement_armijo(f,x,d,m,alpha=0.3,beta=0.5):\n", "    t = 1\n", "    while f(x+t*d) > f(x) + alpha*t*m:\n", "        t = beta*t\n", "    return t\n", "\n", "def gradient_armijo(f,g,x0,err=1e-5,maxiter=2000):\n", "    x = x0.copy()\n", "    fiter = []\n", "    giter = []\n", "    k = 0 # nombre d'it\u00e9rations\n", "    while(True): \n", "        k = k+1\n", "        if k > maxiter: # maximum de 10^6 it\u00e9rations\n", "            print('erreur: nombre maximum d\\'it\u00e9rations atteint')\n", "            break\n", "        d = -g(x)\n", "        fiter.append(f(x))\n", "        giter.append(np.linalg.norm(d))\n", "        if np.linalg.norm(d) <= err:\n", "            break\n", "        t = rebroussement_armijo(f,x,d,-np.linalg.norm(d)**2)\n", "        #if k%10==0: # on affiche des informations toute les 20 it\u00e9rations\n", "        #    print('iteration %d: f=%f, |g|=%f, step=%f' % (k, f(x), np.linalg.norm(d),t))\n", "        x = x + t*d\n", "    return x,np.array(fiter),np.array(giter)\n", "\n", "# <completer>\n"]}], "metadata": {"celltoolbar": "None", "kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.7.6"}}, "nbformat": 4, "nbformat_minor": 1}