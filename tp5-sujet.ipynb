{"cells": [{"cell_type": "markdown", "metadata": {"deletable": false}, "source": ["***\n", "**Algorithmes d'optimisation -- L3 MINT et doubles licences 2019/2020 -- Universit\u00e9 Paris-Sud**\n", "***\n", "\n", "$\\newcommand{\\Rsp}{\\mathbb{R}}\n", "\\newcommand{\\nr}[1]{\\|#1\\|}\n", "\\newcommand{\\abs}[1]{|#1|}\n", "\\newcommand{\\eps}{\\varepsilon}\n", "\\newcommand{\\sca}[2]{\\langle#1|#2\\rangle}\n", "\\newcommand{\\D}{\\mathrm{D}}\n", "\\newcommand{\\hdots}{\\dots}\n", "\\newcommand{\\cond}{\\mathrm{cond}}$\n", "\n", "# TP 5: Projection sur un poly\u00e8dre et application en d\u00e9bruitage\n", "\n", "Dans ce TP, on cherche \u00e0 appliquer l'algorithme d'Uzawa au calcul de la projection d'un point $p\\in \\Rsp^d$ sur un poly\u00e8dre $K$, c'est-\u00e0-dire un ensemble convexe d\u00e9fini par un nombre fini d'in\u00e9galit\u00e9s affines:\n", "\n", "$$ K = \\{ x\\in \\Rsp^d \\mid \\forall 1\\leq i\\leq k, c_i(x) \\leq 0 \\} $$\n", "\n", "o\u00f9 $c_i(x) := \\sca{a_i}{x} - b_i \\leq 0$. \n", "Dans la suite, on note $x\\leq y$ o\u00f9 $x,y$ sont deux vecteurs si $\\forall i, x_i\\leq y_i$. En posant $A$ la \n", "matrice poss\u00e9dant $k$ lignes not\u00e9es $a_1,\\hdots,a_k \\in \\Rsp^d$ et $b\\in\\Rsp^k$, on a donc\n", "\n", "$$ K = \\{ x \\in \\Rsp^d\\mid Ax \\leq b\\} $$\n", "\n", "\n", "##\u00a0Partie I: \u00c9tude et mise en oeuvre de l'algorithme d'Uzawa\n", "\n", "Dans cette premi\u00e8re partie, on donne un algorithme permettant de calculer la projection d'un point $p \\in \\Rsp^d$ sur le poly\u00e8dre $K$:\n", "\n", "$$ (P) := \\min_{x\\in K} \\frac{1}{2} \\nr{x - p}^2 $$\n", "\n", "Le lagrangien $L$ du probl\u00e8me (P) est donn\u00e9 par\n", "\n", "$$L: (x,\\lambda)\\in\\Rsp^d\\times \\Rsp^k_+ \\mapsto f(x) + \\sum_{1\\leq i\\leq k} \\lambda_i c_i(x)$$\n", "\n", "o\u00f9 $f(x) = \\frac{1}{2} \\nr{x - p}^2$, et le probl\u00e8me dual associ\u00e9 \u00e0 (P) est donc\n", "\n", "$$ (D) := \\max_{\\lambda \\in \\Rsp^k_+} \\min_{x\\in \\Rsp^d} L(x,\\lambda) $$\n", "\n", "On rappelle que si $\\lambda^*$ est un maximiseur de (D), alors tout solution du probl\u00e8me de minimisation *sans contrainte* $(P_{\\lambda^*}) = \\min_{x\\in\\Rsp^d} L(x,\\lambda^*)$ est aussi solution du probl\u00e8me (P). En d'autre terme, la connaissance de $\\lambda^*$ permet de remplacer un probl\u00e8me d'optimisation avec contraintes $(P)$ par un probl\u00e8me d'optimisation sans contrainte $(P_{\\lambda^*})$ !\n", "\n", "Nous allons \u00e9tudier dans cette partie l'algorithme d'Uzawa. L'id\u00e9e est de calculer un maximiseur $\\lambda^*$ du probl\u00e8me dual (D) par une m\u00e9thode de gradient projet\u00e9, et de s'en servir pour calculer la solution de (P) en utilisant le dernier rappel. \n", "\n", "**Q1) [Expression du probl\u00e8me dual]** Dans cette question, il s'agit d'\u00e9crire le probl\u00e8me dual de mani\u00e8re plus explicite.\n", "\n", "- Montrer que le lagrangien associ\u00e9 au probl\u00e8me (P) peut s'\u00e9crire $L(x,\\lambda) = \\frac{1}{2}\\nr{x - p}^2  + \\sca{\\lambda}{A x - b}$.\n", "- \u00c9tant donn\u00e9 $\\lambda \\in \\Rsp^k$, donner l'expression de l'unique solution $x_\\lambda$ du probl\u00e8me de minimisation $\\min_{x \\in \\Rsp^d} L(x,\\lambda).$ Pour cela, poser $f_\\lambda(x) = L(x,\\lambda)$ et calculer son miniseur sur $\\Rsp^d$.\n", "- En d\u00e9duire l'expression suivante du probl\u00e8me dual \n", "\n", "$$\\begin{aligned}\n", "&\\qquad (D) := - \\min_{\\lambda \\in M} h(\\lambda) \\\\\n", "&\\hbox{ o\u00f9 } h(\\lambda) =  \\frac{1}{2} \\nr{A^T \\lambda - p}^2 - \\frac{1}{2}\\nr{p}^2 + \\sca{\\lambda}{b} \\hbox{ et } M=\\Rsp_+^k\n", "\\end{aligned}$$\n", "\n", "En particulier, le probl\u00e8me dual est un probl\u00e8me d'optimisation avec contraintes ($x\\in M$), mais l'ensemble de contraintes est tr\u00e8s simple.\n", "\n", "- Montrer que $\\nabla h(\\lambda) = A(A^T \\lambda - p) + b = b - A x_\\lambda$.\n", "\n"]}, {"cell_type": "markdown", "metadata": {"deletable": false}, "source": ["**Algorithme d'Uzawa:** on appelle ainsi l'algorithme du gradient projet\u00e9 pour le probl\u00e8me dual (D):\n", "$$ \\begin{cases}\n", "\\lambda^{(0)} = 0 \\in \\Rsp^k \\\\\n", "g^{(k)} = \\nabla h(\\lambda^{(k)})\\\\\n", "\\lambda^{(k+1)} = p_{\\Rsp_+^k}(\\lambda^{(k)} - \\tau g^{(k)})\\\\\n", "x^{(k+1)} = p - A^T \\lambda^{(k+1)} \\quad (\\in \\arg\\min_{x\\in\\Rsp^d} \\ell(x,\\lambda^{(k+1)}))\n", "\\end{cases}\n", "$$\n", "L'algorithme est arr\u00eat\u00e9 lorsque $\\nr{x^{(k)} - x^{(k+1)}}\\leq \\eps$.\n", "\n", "Pour l'impl\u00e9mentation de l'algorithme, on rappelle que $p_{\\Rsp_+^k}(v) = (\\max(v_1,0),\\hdots,\\max(v_k,0))$.\n", "\n", "\n", "**Q2) [Convergence de l'algorithme d'Uzawa]** On pose  $S_\\tau(\\lambda) := p_{\\Rsp_+^k}(\\lambda - \\tau \\nabla h(\\lambda))$, de sorte que $\\lambda^{(k+1)} = S_\\tau(\\lambda^{(k)})$. \n", "- Montrer que la fonction $h$ est convexe.\n", "- En d\u00e9duire que si $\\lambda^*$ est un point fixe de $S_\\tau$, alors $\\lambda^*$ est solution du probl\u00e8me (D) (i.e. maximise $h$ sur $\\Rsp_+^k$).\n", "\n", "\n", "Ainsi, si la suite $(\\lambda^{(k)})$ converge, sa limite est un maximiseur $\\lambda^*$ de (D) et $x_{\\lambda^*} = x^*$ est un minimiseur de (P)."]}, {"cell_type": "markdown", "metadata": {"deletable": false}, "source": ["\n", "**Q3) [Mise en oeuvre]** Mettre en oeuvre l'algorithme d'Uzawa.\n", "* \u00c9crire une une fonction `projection_convexe(A,b,p,tau,err=1e-6)` calculant les it\u00e9r\u00e9es de $(\\lambda^{(k)}, x^{(k)})$, en arr\u00eatant la boucle d\u00e8s que $\\nr{x^{(k)}- x^{(k+1)}} \\leq$ `err`. \n", "Cette fonction retournera $\\lambda^{(k)}, x^{(k)}$.\n", "* Tester cette fonction sur le convexe $K = \\{ x \\in \\Rsp^2\\mid \\nr{x}_1 \\leq 1 \\}$. On commencera par d\u00e9terminer $A,b$ tel que $K = \\{x \\mid Ax \\leq b \\}$. On v\u00e9rifiera la validit\u00e9 du calcul de deux mani\u00e8res:\n", "  - visuellement, en affichant le segment reliant p \u00e0 son projet\u00e9 q = `projection_convexe(A,b,p,tau)`, pour un assez grand nombre (100) de points p choisis al\u00e9atoirement dans $[-4,4]^2$.\n", "  - en v\u00e9rifiant que la solution $x,\\lambda$ = projection_convexe(A,b,p,tau) satisfait (\u00e0 erreur num\u00e9rique pr\u00e8s) les quatre conditions du th\u00e9or\u00e8me de Karush-Kuhn-Tucker: $Ax \\leq b$ *(admissibilit\u00e9 de $x$)*, $\\lambda \\geq 0$ *(admissibilit\u00e9 de $\\lambda$)*, $\\forall i, (A x - b)_i \\lambda_i = 0$ *(compl\u00e9mentarit\u00e9)* et $\\nabla_x \\ell(x,\\lambda) = 0$ *(optimalit\u00e9)*.\n", "* Recommence avec $K = \\{ x \\in\\Rsp^2 \\mid \\nr{x}_\\infty \\leq 1 \\}$.\n", "\n", "\n"]}, {"cell_type": "code", "execution_count": 1, "metadata": {}, "outputs": [], "source": ["import numpy as np\n", "import matplotlib.pyplot as plt\n", "%matplotlib inline\n", "# la commande suivante agrandit les figures\n", "plt.rcParams['figure.figsize'] = [9.,6.]\n", "\n", "# <completer>\n"]}, {"cell_type": "code", "execution_count": 2, "metadata": {}, "outputs": [], "source": ["# <completer>\n"]}, {"cell_type": "markdown", "metadata": {"deletable": false}, "source": ["## Partie II: R\u00e9gression isotone\n", "\n", "Nous allons consid\u00e9rer deux probl\u00e8mes de d\u00e9bruitage consistant simplement \u00e0 projeter sur un convexe. Les donn\u00e9es sont par exemple des s\u00e9ries temporelles $y = (y_1,\\hdots,y_n)\\in \\Rsp^n$, mesur\u00e9es avec un bruit. On sait que les donn\u00e9es r\u00e9elles appartiennent \u00e0 un certain ensemble convexe $K$ de $\\Rsp^n$. Deux exemples\n", "\n", "- r\u00e9gression isotone: $K = \\{ x\\in \\Rsp^n \\mid \\forall 1\\leq i < n,~x_{i+1}\\geq x_i \\}$\n", "- r\u00e9gression convexe: $K = \\{ x\\in \\Rsp^n \\mid \\forall 1 < i < n,~x_{i} \\leq \\frac{1}{2} (x_{i-1} + x_{i+1}) \\}$.\n", "\n", "\u00c0 cause du bruit, le vecteur $y$ mesur\u00e9 n'appartient pas \u00e0 l'ensemble $K$. L'id\u00e9e est simplement de d\u00e9bruiter le signal en le projetant sur $K$, soit:\n", "\n", "$$ (P) := \\min_{x\\in K} \\frac{1}{2} \\nr{x - p}^2 $$\n", "\n", "**Q1)** Impl\u00e9menter la r\u00e9gression isotone.\n", "- Trouver une matrice $A$ et un vecteur $b$ tel que $K_{iso} = \\{ x\\in \\Rsp^n \\mid Ax \\leq b\u00a0\\}$.\n", "- Utiliser projection_convexe() avec tau=0.1 et avec le vecteur $p$ donn\u00e9 ci-dessous.\n", "- Que peut-on dire exp\u00e9rimentalement de la solution $x^*$ aux points $\\{i,i+1\\}$ o\u00f9 $y_{i}\\geq y_{i+1}$ ? \n", "- D\u00e9montrer ce r\u00e9sultat en utilisant les conditions du th\u00e9or\u00e8me KKT."]}, {"cell_type": "code", "execution_count": 3, "metadata": {}, "outputs": [], "source": ["n = 30\n", "t = np.linspace(0,1,n)\n", "p = t**2 + .3*np.random.rand(n)\n", "\n", "# <completer>\n"]}, {"cell_type": "markdown", "metadata": {"deletable": false}, "source": ["**Q2)** Impl\u00e9menter la r\u00e9gression convexe en utilisant projection_convexe() avec tau=0.1 et avec le vecteur $p$ donn\u00e9 ci-dessous. "]}, {"cell_type": "code", "execution_count": 4, "metadata": {}, "outputs": [], "source": ["p = t**4 + .2*np.random.rand(n)\n", "\n", "# <completer>\n"]}], "metadata": {"celltoolbar": "None", "kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.5.5"}}, "nbformat": 4, "nbformat_minor": 1}