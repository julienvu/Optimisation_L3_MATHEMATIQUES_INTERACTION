{"cells": [{"cell_type": "markdown", "metadata": {"deletable": false}, "source": ["***\n", "**Algorithmes d'optimisation -- L3 MINT et doubles licences 2019/2020 -- Universit\u00e9 Paris-Saclay**\n", "***\n", "\n", "# TP 3 (bis): Projection sur le simplexe et optimisation de portefeuille\n"]}, {"cell_type": "code", "execution_count": 2, "metadata": {}, "outputs": [], "source": ["import numpy as np\n", "import matplotlib.pyplot as plt\n", "import scipy.optimize\n", "%matplotlib inline\n", "# la commande suivante agrandit les figures\n", "plt.rcParams['figure.figsize'] = [9.,6.]\n", "\n", "def verifier_gradient(f,g,x0):\n", "    N = len(x0)\n", "    gg = np.zeros(N)\n", "    for i in range(N):\n", "        eps = 1e-4\n", "        e = np.zeros(N)\n", "        e[i] = eps\n", "        gg[i] = (f(x0+e) - f(x0-e))/(2*eps)\n", "    print('erreur numerique dans le calcul du gradient: %g (doit etre petit)' % np.linalg.norm(g(x0)-gg))"]}, {"cell_type": "markdown", "metadata": {"deletable": false}, "source": ["## I. Projection sur le simplexe\n", "$\\newcommand{\\Rsp}{\\mathbb{R}}$\n", "$\\newcommand{\\sca}[2]{\\langle #1|#2\\rangle}$\n", "$\\newcommand{\\eps}{\\varepsilon}$\n", "$\\newcommand{\\proj}{\\mathrm{proj}}$\n", "Comme dans le troisi\u00e8me exercice de la feuille de TD consacr\u00e9e au calcul de projections, on appelle simplexe l'ensemble \n", "\n", "$$\\Delta = \\{ x\\in \\Rsp_+^n \\mid \\sum_{1\\leq i \\leq n} x_i = 1\\}.$$\n", "\n", "Dans un premier temps, on va chercher \u00e0 calculer la projection d'un point \n", "$y\\in\\Rsp^n$ sur $\\Delta$. Pour cela, on admettra les deux r\u00e9sultats suivants, d\u00e9montr\u00e9s dans le TD:\n", "\n", "- il existe $\\kappa\\in\\Rsp$ tel que $\\sum_{1\\leq i\\leq n} \\max(y_i - \\kappa, 0) = 1$\n", "- la projection de $y$ sur $\\Delta$ s'\u00e9crit alors $\\proj_\\Delta(y) = (\\max(y_i - \\kappa, 0))_{1\\leq i\\leq n}$\n", "\n", "Ainsi, pour trouver la projection d'un point $y\\in\\Rsp^n$ sur le simplexe $\\Delta$, il suffit de trouver $\\kappa\\in\\Rsp$ tel que $g(\\kappa) = 0$ o\u00f9 l'on a pos\u00e9\n", "$$ g(\\kappa) = \\left(\\sum_{1\\leq i\\leq n} \\max(y_i - \\kappa, 0)\\right) -1$$\n", "\n", "\n", "**Q1)** Soit $y = (0.1,1.5,2.1) \\in \\Rsp^3$. \u00c9crire la fonction `g(kappa)` d\u00e9crite ci-dessus. Trouver $\\kappa$ v\u00e9rifiant $g(\\kappa) = 1$ en utilisant la fonction `scipy.optimize.root(g,x0=0,method='anderson').x`."]}, {"cell_type": "code", "execution_count": 3, "metadata": {}, "outputs": [], "source": ["# <completer>\n"]}, {"cell_type": "markdown", "metadata": {"deletable": false}, "source": ["**Q2)** En s'inspirant du code de la fonction pr\u00e9c\u00e9dente, \u00e9crire une fonction proj_simplexe calculant la projection d'un point $y\\in\\Rsp^n$ sur $\\Delta$. Pour v\u00e9rifier le bon fonctionnement de cette fonction, calculer calculer $p=$`proj_simplexe(y)` pour $y=$`np.random.randn(n)` puis v\u00e9rifier que \n", "\n", "$$ \\forall i\\in\\{1,\\dots,n\\}, \\sca{y - p}{p - e_i} \\geq 0,$$\n", "\n", "o\u00f9 $e_i$ est le $i$\u00e8me vecteur de la base canonique.\n", "\n", "*(Question subsidiaire: pourquoi cette in\u00e9galit\u00e9 doit-elle \u00eatre vraie pour $p = \\proj_\\Delta(y)$ ? Caract\u00e9rise-t-elle la projection sur $\\Delta$?)*\n", "<!-- (dans le cas $n=2$, on peut \u00e9galement tirer quelques points al\u00e9atoirement dans le plan et visualiser le segment qui les relie \u00e0 leur projection sur $\\Delta$) -->"]}, {"cell_type": "code", "execution_count": 4, "metadata": {}, "outputs": [], "source": ["def proj_simplexe(y):\n", "    # <completer>\n", "\n", "# validation: calcul de produits scalaires\n", "# <completer>\n"]}, {"cell_type": "markdown", "metadata": {"deletable": false}, "source": ["## II. Optimisation de portefeuille.\n", "\n", "Dans cette partie, il s'agit de r\u00e9soudre un probl\u00e8me d'optimisation sous contraintes de la forme suivante:\n", "\n", "$$ \\min_{x\\in\\Delta} f(x) \\hbox{ o\u00f9 } f(x) = \\frac{1}{2}\\sca{x}{Qx} + \\frac{1}{2\\eta}(\\sca{r}{x}-r_0)^2, $$\n", "\n", "o\u00f9 $Q \\in \\mathcal{M}_{n,n}(\\Rsp)$ est une matrice sym\u00e9trique d\u00e9finie positive, $r\\in\\Rsp$ est un vecteur et $r_0\\in\\Rsp$ sont donn\u00e9s. Dans la suite, on fixera $\\eta = 10^{-2}$. \n", "\n", "**Motivation:** Ce probl\u00e8me mod\u00e9lise une situation o\u00f9 un investisseur cherche placer un portefeuille en garantissant un certain niveau de rendement $r_0\\in\\Rsp$ tout en minimisant le risque. Plus pr\u00e9cis\u00e9ment, dans ce probl\u00e8me $n$ est le nombre d'actifs (actions, etc.), et la variable $x\\in\\Rsp^n$ d\u00e9crit la strat\u00e9gie d'investissement: $x_i$ d\u00e9crit la fraction du portefeuille que l'on investit dans l'actif $i$. Ainsi, la contrainte $x\\in\\Delta$ mod\u00e9lise:\n", "\n", "- qu'on investit une fraction $x_i$ positive dans chaque actif (contrainte $x_i\\geq 0$);\n", "- et que la somme totale disponible est investie ($\\sum_i x_i = 1$).\n", "\n", "La matrice $Q$ mod\u00e9lise la covariance entre les actifs:\n", "- si $Q_{ij}\\geq 0$, cela signifie que les actifs financiers $i$ et $j$ sont positivement correl\u00e9s, et il est donc risqu\u00e9 d'investir dans les deux en m\u00eame temps. \n", "- au contraire, si $Q_{i,j}<0$, les actifs sont n\u00e9gativement correl\u00e9s, et en investissant dans les deux on diminue le risque. \n", "Le probl\u00e8me d'optimisation qu'on regarde consiste donc \u00e0 trouver une strat\u00e9gie d'investissement ($x\\in \\Delta$) cherchant \u00e0 viser un rendement donn\u00e9 (terme $\\frac{1}{2\\eta}(\\sca{r}{x}-r_0)^2$) tout en minimisant le risque (terme $\\sca{x}{Qx}$).\n", "\n", "En pratique, on consid\u00e8rera les donn\u00e9es suivantes:"]}, {"cell_type": "code", "execution_count": 5, "metadata": {"scrolled": true}, "outputs": [], "source": ["# Le code suivant permet de construire la matrice $Q$ \u00e0 partir de donn\u00e9es r\u00e9elles.\n", "# Ne pas h\u00e9siter \u00e0 la d\u00e9commenter et \u00e0 tester avec d'autres choix d'actifs. Pour cela,\n", "# il faut installer le package yfinance en lan\u00e7ant la ligne suivant\n", "# !pip install yfinance\n", "if False: # mettre True pour changer les dates, les actifs, etc\n", "    import yfinance as yf \n", "    import numpy as np\n", "    stocks = ['NFLX','MSFT','BA','AIR']\n", "    closes = np.array([np.array(yf.download(s,'2019-01-01','2020-01-01').Close) for s in stocks]).T\n", "    r = (closes[-1,:] - closes[1,:])/closes[1,:] # rendement sur l'ann\u00e9e 2019\n", "    Q = np.cov(closes - np.mean(closes,0),rowvar=False) # estimation de la covariance entre les valeurs\n", "\n", "Q = np.array([[1199.6242199,  -225.74269344,  270.42617708, -112.31853678],\n", "              [-225.74269344,  224.42514399, -157.75776414,   46.31290714],\n", "              [ 270.42617708, -157.75776414,  600.37115079,  -28.1665365 ],\n", "              [-112.31853678,   46.31290714,  -28.1665365,    21.77422792]])\n", "r = np.array([0.20888442, 0.55953316, 0.00602209, 0.2042723])\n", "r0 = 0.7*np.max(r)\n", "eta = 1e-4"]}, {"cell_type": "markdown", "metadata": {"deletable": false}, "source": ["**Q1)** Montrer que la fonction $f$ est strictement convexe et que\n", "$\\nabla f(x) = Qx + \\frac{1}{\\eta} (\\sca{r}{x}-r_0) r.$"]}, {"cell_type": "markdown", "metadata": {"deletable": false}, "source": ["## II.1. M\u00e9thode de gradient projet\u00e9\n", "\n", "**Q2)** \u00c9crire une fonction `f` et `gradf`, et utiliser la fonction `verifier_gradient` pour valider l'impl\u00e9mentation."]}, {"cell_type": "code", "execution_count": 6, "metadata": {}, "outputs": [], "source": ["# <completer>\n", "\n", "# v\u00e9rification du calcul du gradient\n", "verifier_gradient(f,gradf,np.random.rand(len(r)))"]}, {"cell_type": "markdown", "metadata": {"deletable": false}, "source": ["**Q3)** Impl\u00e9menter l'algorithme du gradient projet\u00e9 pour r\u00e9soudre le probl\u00e8me avec les donn\u00e9es ci-dessus, i.e.\n", "\n", "$$ \\begin{cases} x^{(0)} = 0_{\\Rsp^n}\\\\\n", "x^{(k+1)} = \\mathrm{proj\\_simplexe}(x - \\tau \\nabla f(x^{(k)})) \n", "\\end{cases}$$ \n", "\n", "Tracer sur deux figures distinctes:\n", "- La suite des valeurs $f(x^{(k)})$ pour $1\\leq k<100$.\n", "- La suite $\\|{x^{k} - x^{(k-1)}}\\|$ pour $1\\leq k<100$ (on pourra mettre les abscisses en \u00e9chelle logarithmique). Pourquoi la convergence de cette suite vers z\u00e9ro est-elle une indication du bon fonctionnement de l'algorithme?\n", "\n", "Pour choisir le param\u00e8tre $\\tau$ de l'algorithme du gradient projet\u00e9, on pourra proc\u00e9der par t\u00e2tonnement: par exemple, choisir $\\tau$ de sorte \u00e0 ce que $k\\mapsto f(x^{(k)})$ soit d\u00e9croissante et que la suite $\\|{x^{k} - x^{(k-1)}}\\|$ semble tendre vers z\u00e9ro."]}, {"cell_type": "code", "execution_count": 7, "metadata": {}, "outputs": [], "source": ["# <completer>\n"]}, {"cell_type": "markdown", "metadata": {"deletable": false}, "source": ["## II.2. M\u00e9thode de p\u00e9nalisation\n", "\n", "**Q1** Montrer que le simplexe $\\Delta = \\{ x\\in\\Rsp_+^n\\mid \\sum_i x_i = 1\\}$ peut-\u00eatre mis sous la forme\n", "\n", "$$ \\Delta = \\{ x\\in\\Rsp^n \\mid \\forall 1\\leq i\\leq \\ell, c_i(x) \\leq 0 \\} $$\n", "\n", "avec $\\ell = n+2$ et \n", "\n", "$$ c_i(x) = \\begin{cases} -x_i & \\hbox{ si } 1\\leq i\\leq n \\\\\n", "x_1+\\dots+x_n - 1 & \\hbox{ si } i=n+1\\\\\n", "-(x_1+\\dots+x_n - 1) & \\hbox{ si } i=n+2\\end{cases} $$\n", "\n", "Dans la m\u00e9thode de p\u00e9nalisation, le probl\u00e8me d'optimisation sous contraintes \n", "\n", "$$ P =  \\min_{x\\in\\Delta} f(x) \\hbox{ o\u00f9 } f(x) = \\frac{1}{2}\\sca{x}{Qx} + \\frac{1}{2\\eta}(\\sca{r}{x}-r_0)^2, $$\n", "\n", "est alors approch\u00e9 par le probl\u00e8me d'optimisation sans contraintes suivant:\n", "\n", "$$ P_\\eps = \\min_{x\\in\\Rsp^n} f_\\eps(x) \\hbox{ o\u00f9 } f_\\eps(x) = f(x) + \\frac{1}{\\eps}\\sum_{1\\leq i\\leq \\ell} \\max(c_i(x),0)^2 $$\n", "\n", "**Q2** Montrer que \n", "\n", "$$ f_\\eps(x) = f(x) + \\frac{1}{\\eps}\\left(\\sum_{1\\leq i\\leq n} \\max(-x_i,0)^2)\\right) + \\frac{1}{\\eps}(x_1+\\dots+x_n - 1)^2 $$\n", "$$(\\nabla f_\\eps(x))_i = (\\nabla f(x))_i - \\frac{2}{\\eps} \\max(-x_i,0) + \\frac{2}{\\eps} (x_1+\\dots+x_n - 1)$$\n", "\n", "Coder deux fonctions `feps(x)` et `gradfeps(x)`, et v\u00e9rifier leur bon fonctionnement en utilisant `verifier_gradient`."]}, {"cell_type": "code", "execution_count": 8, "metadata": {}, "outputs": [], "source": ["eps = 1e-3\n", "\n", "# <completer>\n", "\n", "verifier_gradient(feps,gradfeps,10*(np.random.rand(len(r))-.5))"]}, {"cell_type": "markdown", "metadata": {"deletable": false}, "source": ["**Q3** En utilisant la fonction `gradient_armijo` ci-dessous, v\u00e9rifier le fonctionnement de cette approche pour des valeurs de `eps` mod\u00e9r\u00e9es ($\\eps = 10^{-2}$ o\u00f9 $10^{-3}$). Commenter les r\u00e9sultats (respect des contraintes, vitesse de convergence)."]}, {"cell_type": "code", "execution_count": 9, "metadata": {}, "outputs": [], "source": ["def gradient_armijo(f,gradf,x0,err=1e-5,maxiter=20000):\n", "    x = x0.copy()\n", "    fiter = []\n", "    giter = []\n", "    k = 0 # nombre d'it\u00e9rations\n", "    while(True): \n", "        k = k+1\n", "        if k > maxiter: # maximum de 10^6 it\u00e9rations\n", "            print('erreur: nombre maximum d\\'it\u00e9rations atteint')\n", "            break\n", "        d = -gradf(x)\n", "        fiter.append(f(x))\n", "        giter.append(np.linalg.norm(d))\n", "        if np.linalg.norm(d) <= err:\n", "            break\n", "        t = 1\n", "        m = -np.dot(d,d)\n", "        while f(x+t*d) > f(x) + 0.3*t*m:\n", "            t = 0.5*t\n", "        if k%100==0: # on affiche des informations toute les 100 it\u00e9rations\n", "            print('iteration %d: f=%f, |g|=%f, step=%f' % (k, f(x), np.linalg.norm(d),t))\n", "        x = x + t*d\n", "    return x,np.array(fiter),np.array(giter)\n", "\n", "# <completer>\n"]}, {"cell_type": "code", "execution_count": 10, "metadata": {}, "outputs": [{"data": {"text/plain": ["array([0.12850422, 0.48099323, 0.04045281, 0.36423465])"]}, "execution_count": 10, "metadata": {}, "output_type": "execute_result"}], "source": ["x"]}, {"cell_type": "code", "execution_count": 11, "metadata": {}, "outputs": [{"data": {"text/plain": ["1.0141849153713878"]}, "execution_count": 11, "metadata": {}, "output_type": "execute_result"}], "source": ["np.sum(x)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.7.3"}, "celltoolbar": "None"}, "nbformat": 4, "nbformat_minor": 2}